from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from datasets import load_dataset
import torch

# ========== é…ç½®å‚æ•° ==========
MODEL_NAME = "Qwen/Qwen2-1.5B-Instruct"  # æ¨èä½¿ç”¨Qwenç³»åˆ—
OUTPUT_DIR = "./css_assistant_model"
MAX_LENGTH = 512  # CSSç›¸å…³é—®ç­”é€šå¸¸ä¸éœ€è¦å¤ªé•¿

print("ğŸš€ å¼€å§‹å¾®è°ƒCSSç±»ååŠ©æ‰‹...")

# ========== åŠ è½½æ¨¡å‹ ==========
print("\nğŸ“¥ åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")
tokenizer = AutoTokenizer.from_pretrained(
    MODEL_NAME, trust_remote_code=True, padding_side="right"  # é‡è¦ï¼šè®¾ç½®paddingæ–¹å‘
)

# è®¾ç½®pad_token
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME, torch_dtype=torch.float16, device_map="auto", trust_remote_code=True
)

# ========== é…ç½®LoRA ==========
print("\nâš™ï¸ é…ç½®LoRA...")
lora_config = LoraConfig(
    r=16,  # å¢åŠ ç§©ä»¥æé«˜æ€§èƒ½
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],  # æ›´å¤šç›®æ ‡æ¨¡å—
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, lora_config)

print("ğŸ“Š å¯è®­ç»ƒå‚æ•°:")
model.print_trainable_parameters()

# ========== åŠ è½½æ•°æ® ==========
print("\nğŸ“‚ åŠ è½½è®­ç»ƒæ•°æ®...")
dataset = load_dataset("json", data_files="training_data.json", split="train")

print(f"âœ… åŠ è½½äº† {len(dataset)} æ¡è®­ç»ƒæ•°æ®")


# ========== æ•°æ®é¢„å¤„ç† ==========
def format_prompt(example):
    """æ ¼å¼åŒ–ä¸ºå¯¹è¯æ ¼å¼"""
    prompt = f"""<|im_start|>system
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„CSSç±»ååŠ©æ‰‹ï¼Œå¸®åŠ©å¼€å‘è€…å¿«é€ŸæŸ¥æ‰¾å’Œä½¿ç”¨Tailwindé£æ ¼çš„CSSç±»åã€‚ä½ çš„å›ç­”åº”è¯¥ç®€æ´ã€å‡†ç¡®ã€‚<|im_end|>
<|im_start|>user
{example['instruction']}<|im_end|>
<|im_start|>assistant
{example['output']}<|im_end|>"""
    return {"text": prompt}


print("\nğŸ”„ é¢„å¤„ç†æ•°æ®...")
dataset = dataset.map(format_prompt, remove_columns=dataset.column_names)


def tokenize_function(examples):
    result = tokenizer(
        examples["text"],
        padding="max_length",
        truncation=True,
        max_length=MAX_LENGTH,
        return_tensors=None,
    )
    result["labels"] = result["input_ids"].copy()
    return result


tokenized_dataset = dataset.map(
    tokenize_function, batched=True, remove_columns=["text"]
)

# ========== è®­ç»ƒé…ç½® ==========
print("\nâš™ï¸ é…ç½®è®­ç»ƒå‚æ•°...")
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    num_train_epochs=5,  # CSSæ•°æ®è¾ƒå°‘ï¼Œå¤šè®­ç»ƒå‡ è½®
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    lr_scheduler_type="cosine",
    warmup_steps=100,
    logging_steps=10,
    save_steps=50,
    save_total_limit=3,
    fp16=True,
    optim="adamw_torch",
    report_to="none",  # ä¸ä½¿ç”¨wandbç­‰å·¥å…·
)

# ========== å¼€å§‹è®­ç»ƒ ==========
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer,
    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),
)

print("\nğŸ¯ å¼€å§‹è®­ç»ƒ...")
print("=" * 60)
trainer.train()

# ========== ä¿å­˜æ¨¡å‹ ==========
print("\nğŸ’¾ ä¿å­˜æ¨¡å‹...")
model.save_pretrained(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)

print(f"\nâœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ°: {OUTPUT_DIR}")
print("\nä¸‹ä¸€æ­¥: è¿è¡Œ test_model.py æµ‹è¯•æ¨¡å‹æ•ˆæœ")
