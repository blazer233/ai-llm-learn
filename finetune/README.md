# CSS åŠ©æ‰‹æ¨¡å‹å¾®è°ƒé¡¹ç›®

> åŸºäº Qwen2.5-3B-Instruct å¾®è°ƒçš„ CSS ä»£ç ç”ŸæˆåŠ©æ‰‹

## ğŸ“‹ ç›®å½•

- [é¡¹ç›®ç®€ä»‹](#é¡¹ç›®ç®€ä»‹)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [ç›®å½•ç»“æ„](#ç›®å½•ç»“æ„)
- [ä½¿ç”¨æŒ‡å—](#ä½¿ç”¨æŒ‡å—)
- [è„šæœ¬è¯´æ˜](#è„šæœ¬è¯´æ˜)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## ğŸ“– é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®ä½¿ç”¨ **LoRA (Low-Rank Adaptation)** æŠ€æœ¯å¯¹ Qwen2.5-3B-Instruct æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œè®­ç»ƒä¸€ä¸ªä¸“é—¨ç”¨äºç”Ÿæˆ CSS ä»£ç çš„åŠ©æ‰‹ã€‚

### æ ¸å¿ƒç‰¹æ€§

- âœ… **è½»é‡çº§å¾®è°ƒ**: ä½¿ç”¨ LoRA æŠ€æœ¯ï¼Œä»…è®­ç»ƒ 0.48% çš„å‚æ•°
- âœ… **é«˜è´¨é‡æ•°æ®**: 10,406 æ¡ç²¾å¿ƒå¤„ç†çš„è®­ç»ƒæ ·æœ¬
- âœ… **æ˜“äºä½¿ç”¨**: æä¾›å®Œæ•´çš„è‡ªåŠ¨åŒ–è„šæœ¬
- âœ… **å¯æ¢å¤è®­ç»ƒ**: æ”¯æŒè®­ç»ƒä¸­æ–­åæ¢å¤
- âœ… **è¯¦ç»†æ–‡æ¡£**: å®Œæ•´çš„ä½¿ç”¨è¯´æ˜å’Œæ•…éšœæ’é™¤æŒ‡å—

### æŠ€æœ¯æ ˆ

- **åŸºåº§æ¨¡å‹**: Qwen/Qwen2.5-3B-Instruct (3.09B å‚æ•°)
- **å¾®è°ƒæ–¹æ³•**: LoRA (r=8, alpha=16)
- **æ¡†æ¶**: Transformers + PEFT + Datasets
- **ç¡¬ä»¶**: æ”¯æŒ CPU/MPS/CUDA

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

- Python 3.10 æˆ– 3.11
- 8GB+ å†…å­˜ï¼ˆæ¨è 16GB+ï¼‰
- 10GB+ ç£ç›˜ç©ºé—´

### ä¸€é”®å¯åŠ¨è®­ç»ƒ

```bash
cd /Users/songyanchao/Desktop/thing/zhishi/finetune
./scripts/setup_and_train.sh
```

è¿™ä¸ªè„šæœ¬ä¼šè‡ªåŠ¨ï¼š
1. æ£€æŸ¥ Python ç¯å¢ƒ
2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
3. å®‰è£…ä¾èµ–åŒ…
4. å¼€å§‹è®­ç»ƒ

### è®­ç»ƒæ—¶é—´

- **M1/M2 Mac (32GB)**: çº¦ 2-4 å°æ—¶
- **CPU**: çº¦ 6-12 å°æ—¶
- **GPU (CUDA)**: çº¦ 1-2 å°æ—¶

---

## ğŸ“ ç›®å½•ç»“æ„

```
finetune/
â”œâ”€â”€ README.md                    # æœ¬æ–‡ä»¶ - é¡¹ç›®æ€»è§ˆ
â”œâ”€â”€ docs/                        # ğŸ“š æ–‡æ¡£ç›®å½•
â”‚   â”œâ”€â”€ TRAINING_STATUS.md      # è®­ç»ƒçŠ¶æ€å’Œè¯¦ç»†è¯´æ˜
â”‚   â””â”€â”€ README_TRAINING.md      # è®­ç»ƒæŒ‡å—ï¼ˆæ—§ç‰ˆï¼‰
â”œâ”€â”€ scripts/                     # ğŸ”§ è„šæœ¬ç›®å½•
â”‚   â”œâ”€â”€ setup_and_train.sh      # ä¸€é”®å®‰è£…å’Œè®­ç»ƒ
â”‚   â”œâ”€â”€ monitor_training.sh     # ç›‘æ§è®­ç»ƒè¿›åº¦
â”‚   â”œâ”€â”€ pause_training.sh       # æš‚åœè®­ç»ƒ
â”‚   â”œâ”€â”€ resume_training.sh      # æ¢å¤è®­ç»ƒ
â”‚   â””â”€â”€ start_training.sh       # å¯åŠ¨è®­ç»ƒï¼ˆæ—§ç‰ˆï¼‰
â”œâ”€â”€ simple_train.py              # ğŸ¯ ä¸»è®­ç»ƒè„šæœ¬
â”œâ”€â”€ process_data.py              # ğŸ“Š æ•°æ®å¤„ç†è„šæœ¬
â”œâ”€â”€ check_data_quality.py        # âœ… æ•°æ®è´¨é‡æ£€æŸ¥
â”œâ”€â”€ training_data.json           # ğŸ“¦ è®­ç»ƒæ•°æ® (10,406 æ¡)
â”œâ”€â”€ css_classes.json             # ğŸ¨ CSS ç±»å®šä¹‰
â”œâ”€â”€ train_config.yaml            # âš™ï¸ è®­ç»ƒé…ç½®ï¼ˆLLaMA-Factoryï¼‰
â”œâ”€â”€ train_cli.py                 # ğŸ’» å‘½ä»¤è¡Œè®­ç»ƒï¼ˆLLaMA-Factoryï¼‰
â”œâ”€â”€ finetune_css.py              # ğŸ”§ å¾®è°ƒè„šæœ¬ï¼ˆæ—§ç‰ˆï¼‰
â”œâ”€â”€ train_env/                   # ğŸ Python è™šæ‹Ÿç¯å¢ƒ
â”œâ”€â”€ css_assistant_model/         # ğŸ’¾ è®­ç»ƒè¾“å‡ºç›®å½•
â”‚   â”œâ”€â”€ checkpoint-*/           # è®­ç»ƒæ£€æŸ¥ç‚¹
â”‚   â”œâ”€â”€ adapter_model.safetensors # LoRA æƒé‡
â”‚   â””â”€â”€ adapter_config.json     # LoRA é…ç½®
â””â”€â”€ LLaMA-Factory/              # ğŸ­ LLaMA-Factory æ¡†æ¶ï¼ˆå¯é€‰ï¼‰
```

### ç›®å½•è¯´æ˜

#### ğŸ“š `docs/` - æ–‡æ¡£ç›®å½•
å­˜æ”¾æ‰€æœ‰é¡¹ç›®æ–‡æ¡£å’Œè¯´æ˜æ–‡ä»¶

#### ğŸ”§ `scripts/` - è„šæœ¬ç›®å½•
æ‰€æœ‰å¯æ‰§è¡Œçš„ Shell è„šæœ¬ï¼Œç”¨äºè®­ç»ƒç®¡ç†

#### ğŸ’¾ `css_assistant_model/` - æ¨¡å‹è¾“å‡º
è®­ç»ƒå®Œæˆåçš„æ¨¡å‹æ–‡ä»¶å’Œæ£€æŸ¥ç‚¹

#### ğŸ è™šæ‹Ÿç¯å¢ƒç›®å½•ï¼ˆå·²å¿½ç•¥ï¼‰
- `train_env/` - è®­ç»ƒç¯å¢ƒ
- `llama_env/` - LLaMA-Factory ç¯å¢ƒ
- `venv/` - å…¶ä»–è™šæ‹Ÿç¯å¢ƒ

---

## ğŸ“– ä½¿ç”¨æŒ‡å—

### 1. æ•°æ®å‡†å¤‡

#### æŸ¥çœ‹æ•°æ®è´¨é‡
```bash
python check_data_quality.py
```

#### é‡æ–°å¤„ç†æ•°æ®
```bash
python process_data.py
```

### 2. è®­ç»ƒæ¨¡å‹

#### æ–¹å¼ä¸€ï¼šè‡ªåŠ¨åŒ–è®­ç»ƒï¼ˆæ¨èï¼‰
```bash
./scripts/setup_and_train.sh
```

#### æ–¹å¼äºŒï¼šæ‰‹åŠ¨è®­ç»ƒ
```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python3.11 -m venv train_env
source train_env/bin/activate

# å®‰è£…ä¾èµ–
pip install torch transformers peft datasets accelerate

# å¼€å§‹è®­ç»ƒ
python simple_train.py
```

### 3. è®­ç»ƒç®¡ç†

#### ç›‘æ§è®­ç»ƒ
```bash
./scripts/monitor_training.sh
```

è¾“å‡ºç¤ºä¾‹ï¼š
```
âœ“ è®­ç»ƒè¿›ç¨‹æ­£åœ¨è¿è¡Œ
è¿›ç¨‹ä¿¡æ¯:
  PID: 12345
  CPU: 97.3%
  å†…å­˜: 0.5%
  è¿è¡Œæ—¶é—´: 2:03.04
```

#### æš‚åœè®­ç»ƒ
```bash
./scripts/pause_training.sh
```

#### æ¢å¤è®­ç»ƒ
```bash
./scripts/resume_training.sh
```

#### æŸ¥çœ‹æ—¥å¿—
```bash
# å®æ—¶æŸ¥çœ‹
tail -f training.log

# æŸ¥çœ‹æœ€å 50 è¡Œ
tail -50 training.log

# æœç´¢é”™è¯¯
grep -i error training.log
```

### 4. ä½¿ç”¨æ¨¡å‹

#### åŠ è½½æ¨¡å‹
```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# åŠ è½½åŸºåº§æ¨¡å‹
base_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-3B-Instruct",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-3B-Instruct")

# åŠ è½½ LoRA æƒé‡
model = PeftModel.from_pretrained(base_model, "css_assistant_model")
model.eval()
```

#### ç”Ÿæˆ CSS
```python
def generate_css(prompt):
    # æ ¼å¼åŒ–è¾“å…¥
    text = f"<|im_start|>system\nä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ CSS åŠ©æ‰‹ã€‚<|im_end|>\n<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
    
    # åˆ†è¯
    inputs = tokenizer(text, return_tensors="pt").to(model.device)
    
    # ç”Ÿæˆ
    outputs = model.generate(
        **inputs,
        max_length=512,
        temperature=0.7,
        top_p=0.9,
        do_sample=True
    )
    
    # è§£ç 
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return result

# æµ‹è¯•
prompt = "ç”Ÿæˆä¸€ä¸ªå±…ä¸­çš„çº¢è‰²æ–‡å­—çš„ CSS ç±»"
print(generate_css(prompt))
```

---

## ğŸ”§ è„šæœ¬è¯´æ˜

### `scripts/setup_and_train.sh`
**ä¸€é”®å®‰è£…å’Œè®­ç»ƒè„šæœ¬**

åŠŸèƒ½ï¼š
- æ£€æŸ¥ Python ç¯å¢ƒ
- åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
- å®‰è£…ä¾èµ–åŒ…
- å¯åŠ¨è®­ç»ƒ

ä½¿ç”¨ï¼š
```bash
./scripts/setup_and_train.sh
```

### `scripts/monitor_training.sh`
**è®­ç»ƒç›‘æ§è„šæœ¬**

åŠŸèƒ½ï¼š
- æ˜¾ç¤ºè®­ç»ƒè¿›ç¨‹çŠ¶æ€
- æ˜¾ç¤º CPU/å†…å­˜ä½¿ç”¨
- åˆ—å‡ºå·²ä¿å­˜çš„ checkpoints

ä½¿ç”¨ï¼š
```bash
./scripts/monitor_training.sh
```

### `scripts/pause_training.sh`
**æš‚åœè®­ç»ƒè„šæœ¬**

åŠŸèƒ½ï¼š
- å®‰å…¨åœæ­¢è®­ç»ƒè¿›ç¨‹
- æ˜¾ç¤ºå·²ä¿å­˜çš„ checkpoints
- æä¾›æ¢å¤è®­ç»ƒæŒ‡å¼•

ä½¿ç”¨ï¼š
```bash
./scripts/pause_training.sh
```

### `scripts/resume_training.sh`
**æ¢å¤è®­ç»ƒè„šæœ¬**

åŠŸèƒ½ï¼š
- æ£€æŸ¥æ˜¯å¦æœ‰ checkpoint
- åœ¨åå°å¯åŠ¨è®­ç»ƒ
- æ—¥å¿—è¾“å‡ºåˆ° `training.log`

ä½¿ç”¨ï¼š
```bash
./scripts/resume_training.sh
```

---

## â“ å¸¸è§é—®é¢˜

### Q1: è®­ç»ƒéœ€è¦å¤šé•¿æ—¶é—´ï¼Ÿ
**A**: å–å†³äºç¡¬ä»¶é…ç½®ï¼š
- M1/M2 Mac (32GB): 2-4 å°æ—¶
- CPU: 6-12 å°æ—¶
- GPU (CUDA): 1-2 å°æ—¶

### Q2: è®­ç»ƒä¸­æ–­äº†æ€ä¹ˆåŠï¼Ÿ
**A**: 
1. æ£€æŸ¥æ˜¯å¦æœ‰ä¿å­˜çš„ checkpointï¼š`ls css_assistant_model/checkpoint-*`
2. å¦‚æœæœ‰ checkpointï¼Œå¯ä»¥ä¿®æ”¹ `simple_train.py` æ·»åŠ æ¢å¤åŠŸèƒ½
3. å¦‚æœæ²¡æœ‰ï¼Œéœ€è¦ä»å¤´å¼€å§‹è®­ç»ƒ

### Q3: å†…å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ
**A**: ä¿®æ”¹ `simple_train.py` ä¸­çš„å‚æ•°ï¼š
```python
BATCH_SIZE = 2  # å‡å°æ‰¹æ¬¡å¤§å°
GRADIENT_ACCUMULATION_STEPS = 8  # å¢åŠ æ¢¯åº¦ç´¯ç§¯
MAX_LENGTH = 256  # å‡å°æœ€å¤§åºåˆ—é•¿åº¦
```

### Q4: å¦‚ä½•ä¿®æ”¹è®­ç»ƒå‚æ•°ï¼Ÿ
**A**: ç¼–è¾‘ `simple_train.py` æ–‡ä»¶ï¼Œä¿®æ”¹ä»¥ä¸‹å‚æ•°ï¼š
```python
# LoRA é…ç½®
LORA_R = 8  # LoRA ç§©
LORA_ALPHA = 16  # LoRA alpha
LORA_DROPOUT = 0.05  # Dropout

# è®­ç»ƒé…ç½®
BATCH_SIZE = 4  # æ‰¹æ¬¡å¤§å°
GRADIENT_ACCUMULATION_STEPS = 4  # æ¢¯åº¦ç´¯ç§¯
LEARNING_RATE = 2e-4  # å­¦ä¹ ç‡
NUM_EPOCHS = 3  # è®­ç»ƒè½®æ•°
```

### Q5: è®­ç»ƒå®Œæˆåå¦‚ä½•ä½¿ç”¨æ¨¡å‹ï¼Ÿ
**A**: å‚è€ƒ [ä½¿ç”¨æŒ‡å— - ä½¿ç”¨æ¨¡å‹](#4-ä½¿ç”¨æ¨¡å‹) éƒ¨åˆ†

### Q6: å¯ä»¥åœ¨äº‘ç«¯è®­ç»ƒå—ï¼Ÿ
**A**: å¯ä»¥ï¼æ¨èå¹³å°ï¼š
- Google Colab (å…è´¹ GPU)
- AWS SageMaker
- é˜¿é‡Œäº‘ PAI
- è…¾è®¯äº‘ TI-ONE

### Q7: å¦‚ä½•è¯„ä¼°æ¨¡å‹æ•ˆæœï¼Ÿ
**A**: 
1. ä½¿ç”¨æµ‹è¯•é›†è¯„ä¼°
2. äººå·¥è¯„ä¼°ç”Ÿæˆçš„ CSS ä»£ç 
3. å¯¹æ¯”å¾®è°ƒå‰åçš„æ•ˆæœ

---

## ğŸ“Š è®­ç»ƒæ•°æ®è¯´æ˜

### æ•°æ®ç»Ÿè®¡
- **æ€»æ ·æœ¬æ•°**: 10,406 æ¡
- **æ­£æ ·æœ¬**: 9,585 æ¡ (92.1%)
- **è´Ÿæ ·æœ¬**: 821 æ¡ (7.9%)

### æ•°æ®æ ¼å¼
```json
{
  "instruction": "ç”Ÿæˆä¸€ä¸ªå±…ä¸­çš„çº¢è‰²æ–‡å­—çš„ CSS ç±»",
  "input": "",
  "output": ".centered-red-text {\n  text-align: center;\n  color: red;\n}"
}
```

### æ•°æ®æ¥æº
- CSS ç±»å®šä¹‰ (`css_classes.json`)
- æ•°æ®å¢å¼ºå’Œå˜æ¢
- è´Ÿæ ·æœ¬ç”Ÿæˆ

---

## ğŸ”— ç›¸å…³èµ„æº

### æ–‡æ¡£
- [è®­ç»ƒçŠ¶æ€æ–‡æ¡£](docs/TRAINING_STATUS.md)
- [è®­ç»ƒæŒ‡å—](docs/README_TRAINING.md)

### æ¨¡å‹
- [Qwen2.5-3B-Instruct](https://huggingface.co/Qwen/Qwen2.5-3B-Instruct)

### æ¡†æ¶
- [Transformers](https://github.com/huggingface/transformers)
- [PEFT](https://github.com/huggingface/peft)
- [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)

---

## ğŸ“ æ›´æ–°æ—¥å¿—

### 2025-11-11
- âœ… åˆ›å»ºé¡¹ç›®ç»“æ„
- âœ… æ•´ç†è„šæœ¬åˆ° `scripts/` ç›®å½•
- âœ… æ•´ç†æ–‡æ¡£åˆ° `docs/` ç›®å½•
- âœ… åˆ›å»ºè¯¦ç»†çš„ README æ–‡æ¡£
- âœ… æ·»åŠ è®­ç»ƒç®¡ç†è„šæœ¬

---

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®ä»…ä¾›å­¦ä¹ å’Œç ”ç©¶ä½¿ç”¨ã€‚

---

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æå‡ºé—®é¢˜å’Œå»ºè®®ï¼

---

**ç¥æ‚¨è®­ç»ƒé¡ºåˆ©ï¼** ğŸ‰
