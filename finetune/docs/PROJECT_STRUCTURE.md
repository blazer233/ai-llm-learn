# é¡¹ç›®ç»“æ„è¯´æ˜

> è¯¦ç»†çš„ç›®å½•å’Œæ–‡ä»¶è¯´æ˜

## ğŸ“ å®Œæ•´ç›®å½•æ ‘

```
finetune/
â”‚
â”œâ”€â”€ ğŸ“„ README.md                    # é¡¹ç›®æ€»è§ˆå’Œä½¿ç”¨æŒ‡å—
â”‚
â”œâ”€â”€ ğŸ“š docs/                        # æ–‡æ¡£ç›®å½•
â”‚   â”œâ”€â”€ QUICK_START.md             # å¿«é€Ÿå¼€å§‹æŒ‡å—
â”‚   â”œâ”€â”€ SCRIPTS_GUIDE.md           # è„šæœ¬ä½¿ç”¨æŒ‡å—
â”‚   â”œâ”€â”€ PROJECT_STRUCTURE.md       # æœ¬æ–‡ä»¶ - é¡¹ç›®ç»“æ„è¯´æ˜
â”‚   â”œâ”€â”€ TRAINING_STATUS.md         # è®­ç»ƒçŠ¶æ€å’Œè¯¦ç»†è¯´æ˜
â”‚   â””â”€â”€ README_TRAINING.md         # è®­ç»ƒæŒ‡å—ï¼ˆæ—§ç‰ˆï¼‰
â”‚
â”œâ”€â”€ ğŸ”§ scripts/                     # è„šæœ¬ç›®å½•
â”‚   â”œâ”€â”€ setup_and_train.sh         # ä¸€é”®å®‰è£…å’Œè®­ç»ƒ
â”‚   â”œâ”€â”€ monitor_training.sh        # ç›‘æ§è®­ç»ƒè¿›åº¦
â”‚   â”œâ”€â”€ pause_training.sh          # æš‚åœè®­ç»ƒ
â”‚   â”œâ”€â”€ resume_training.sh         # æ¢å¤è®­ç»ƒ
â”‚   â””â”€â”€ start_training.sh          # å¯åŠ¨è®­ç»ƒï¼ˆæ—§ç‰ˆï¼‰
â”‚
â”œâ”€â”€ ğŸ¯ simple_train.py              # ä¸»è®­ç»ƒè„šæœ¬
â”œâ”€â”€ ğŸ“Š process_data.py              # æ•°æ®å¤„ç†è„šæœ¬
â”œâ”€â”€ âœ… check_data_quality.py        # æ•°æ®è´¨é‡æ£€æŸ¥
â”œâ”€â”€ ğŸ”§ finetune_css.py              # å¾®è°ƒè„šæœ¬ï¼ˆæ—§ç‰ˆï¼‰
â”‚
â”œâ”€â”€ ğŸ“¦ training_data.json           # è®­ç»ƒæ•°æ® (10,406 æ¡)
â”œâ”€â”€ ğŸ¨ css_classes.json             # CSS ç±»å®šä¹‰
â”‚
â”œâ”€â”€ âš™ï¸ train_config.yaml            # è®­ç»ƒé…ç½®ï¼ˆLLaMA-Factoryï¼‰
â”œâ”€â”€ ğŸ’» train_cli.py                 # å‘½ä»¤è¡Œè®­ç»ƒï¼ˆLLaMA-Factoryï¼‰
â”‚
â”œâ”€â”€ ğŸ’¾ css_assistant_model/         # è®­ç»ƒè¾“å‡ºç›®å½•
â”‚   â”œâ”€â”€ checkpoint-500/            # ç¬¬ 500 æ­¥æ£€æŸ¥ç‚¹
â”‚   â”œâ”€â”€ checkpoint-1000/           # ç¬¬ 1000 æ­¥æ£€æŸ¥ç‚¹
â”‚   â”œâ”€â”€ checkpoint-1500/           # ç¬¬ 1500 æ­¥æ£€æŸ¥ç‚¹
â”‚   â”œâ”€â”€ adapter_model.safetensors  # LoRA æƒé‡æ–‡ä»¶
â”‚   â”œâ”€â”€ adapter_config.json        # LoRA é…ç½®
â”‚   â”œâ”€â”€ tokenizer_config.json      # åˆ†è¯å™¨é…ç½®
â”‚   â””â”€â”€ tokenizer.json             # åˆ†è¯å™¨
â”‚
â”œâ”€â”€ ğŸ“ training.log                 # è®­ç»ƒæ—¥å¿—ï¼ˆåå°è¿è¡Œæ—¶ï¼‰
â”‚
â”œâ”€â”€ ğŸ train_env/                   # Python è™šæ‹Ÿç¯å¢ƒï¼ˆå·²å¿½ç•¥ï¼‰
â”œâ”€â”€ ğŸ llama_env/                   # LLaMA-Factory ç¯å¢ƒï¼ˆå·²å¿½ç•¥ï¼‰
â”œâ”€â”€ ğŸ venv/                        # å…¶ä»–è™šæ‹Ÿç¯å¢ƒï¼ˆå·²å¿½ç•¥ï¼‰
â”‚
â””â”€â”€ ğŸ­ LLaMA-Factory/               # LLaMA-Factory æ¡†æ¶ï¼ˆå·²å¿½ç•¥ï¼‰
    â””â”€â”€ ...
```

---

## ğŸ“„ æ ¸å¿ƒæ–‡ä»¶è¯´æ˜

### æ–‡æ¡£æ–‡ä»¶

#### `README.md`
- **ä½œç”¨**: é¡¹ç›®æ€»è§ˆå’Œå®Œæ•´ä½¿ç”¨æŒ‡å—
- **å†…å®¹**: 
  - é¡¹ç›®ç®€ä»‹
  - å¿«é€Ÿå¼€å§‹
  - ç›®å½•ç»“æ„
  - ä½¿ç”¨æŒ‡å—
  - è„šæœ¬è¯´æ˜
  - å¸¸è§é—®é¢˜
- **é€‚åˆ**: é¦–æ¬¡ä½¿ç”¨è€…å’Œå®Œæ•´å‚è€ƒ

#### `docs/QUICK_START.md`
- **ä½œç”¨**: 5 åˆ†é’Ÿå¿«é€Ÿä¸Šæ‰‹æŒ‡å—
- **å†…å®¹**: 
  - ä¸‰æ­¥å¼€å§‹è®­ç»ƒ
  - å¸¸ç”¨å‘½ä»¤é€ŸæŸ¥
  - å¿«é€Ÿæµ‹è¯•æ¨¡å‹
- **é€‚åˆ**: å¿«é€Ÿå¼€å§‹è®­ç»ƒ

#### `docs/SCRIPTS_GUIDE.md`
- **ä½œç”¨**: è¯¦ç»†çš„è„šæœ¬ä½¿ç”¨è¯´æ˜
- **å†…å®¹**: 
  - æ¯ä¸ªè„šæœ¬çš„åŠŸèƒ½
  - ä½¿ç”¨æ–¹æ³•å’Œç¤ºä¾‹
  - å·¥ä½œæµç¨‹
  - æ•…éšœæ’é™¤
- **é€‚åˆ**: æ·±å…¥äº†è§£è„šæœ¬åŠŸèƒ½

#### `docs/TRAINING_STATUS.md`
- **ä½œç”¨**: è®­ç»ƒçŠ¶æ€å’Œè¯¦ç»†é…ç½®
- **å†…å®¹**: 
  - å½“å‰è®­ç»ƒçŠ¶æ€
  - è®­ç»ƒé…ç½®è¯¦æƒ…
  - ç›‘æ§æ–¹æ³•
  - é¢„è®¡æ—¶é—´
  - å®Œæˆåæ“ä½œ
- **é€‚åˆ**: è®­ç»ƒè¿‡ç¨‹ä¸­å‚è€ƒ

---

### Python è„šæœ¬

#### `simple_train.py` â­
- **ä½œç”¨**: ä¸»è®­ç»ƒè„šæœ¬
- **åŠŸèƒ½**: 
  - åŠ è½½è®­ç»ƒæ•°æ®
  - åŠ è½½ Qwen2.5-3B æ¨¡å‹
  - é…ç½® LoRA
  - æ‰§è¡Œè®­ç»ƒ
  - ä¿å­˜æ¨¡å‹
- **é…ç½®å‚æ•°**:
  ```python
  MODEL_NAME = "Qwen/Qwen2.5-3B-Instruct"
  LORA_R = 8
  LORA_ALPHA = 16
  BATCH_SIZE = 4
  LEARNING_RATE = 2e-4
  NUM_EPOCHS = 3
  ```
- **è¾“å‡º**: `css_assistant_model/`

#### `process_data.py`
- **ä½œç”¨**: æ•°æ®å¤„ç†å’Œå¢å¼º
- **åŠŸèƒ½**: 
  - è¯»å– CSS ç±»å®šä¹‰
  - æ•°æ®å¢å¼ºï¼ˆ5 ç§æ¨¡æ¿ï¼‰
  - è´Ÿæ ·æœ¬ç”Ÿæˆï¼ˆ10 ç§æ¨¡æ¿ Ã— 80 ä¸ªé—®é¢˜ï¼‰
  - æ•°æ®å»é‡
  - æ•°æ®æ‰“ä¹±
- **è¾“å…¥**: `css_classes.json`
- **è¾“å‡º**: `training_data.json`

#### `check_data_quality.py`
- **ä½œç”¨**: æ•°æ®è´¨é‡æ£€æŸ¥
- **åŠŸèƒ½**: 
  - ç»Ÿè®¡æ ·æœ¬æ•°é‡
  - åˆ†ææ­£è´Ÿæ ·æœ¬æ¯”ä¾‹
  - æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
  - åˆ†æé•¿åº¦åˆ†å¸ƒ
  - æ£€æµ‹é‡å¤é—®é¢˜
- **è¾“å‡º**: æ§åˆ¶å°æŠ¥å‘Š

#### `finetune_css.py`
- **ä½œç”¨**: æ—©æœŸç‰ˆæœ¬çš„å¾®è°ƒè„šæœ¬
- **çŠ¶æ€**: å·²è¢« `simple_train.py` æ›¿ä»£
- **ä¿ç•™åŸå› **: å‚è€ƒå’Œå¤‡ä»½

#### `train_cli.py`
- **ä½œç”¨**: LLaMA-Factory å‘½ä»¤è¡Œè®­ç»ƒ
- **åŠŸèƒ½**: ä½¿ç”¨ LLaMA-Factory æ¡†æ¶è®­ç»ƒ
- **é…ç½®**: `train_config.yaml`
- **çŠ¶æ€**: å¯é€‰æ–¹æ¡ˆ

---

### æ•°æ®æ–‡ä»¶

#### `training_data.json` (1.3 MB)
- **æ ¼å¼**: JSON æ•°ç»„
- **æ ·æœ¬æ•°**: 10,406 æ¡
- **ç»“æ„**:
  ```json
  {
    "instruction": "é—®é¢˜æˆ–æŒ‡ä»¤",
    "input": "é¢å¤–è¾“å…¥ï¼ˆå¯é€‰ï¼‰",
    "output": "æœŸæœ›è¾“å‡º"
  }
  ```
- **ç¤ºä¾‹**:
  ```json
  {
    "instruction": "ç”Ÿæˆä¸€ä¸ªå±…ä¸­çš„çº¢è‰²æ–‡å­—çš„ CSS ç±»",
    "input": "",
    "output": ".centered-red-text {\n  text-align: center;\n  color: red;\n}"
  }
  ```

#### `css_classes.json` (145 KB)
- **æ ¼å¼**: JSON å¯¹è±¡
- **å†…å®¹**: CSS ç±»åå’Œå®šä¹‰
- **ç”¨é€”**: æ•°æ®å¤„ç†çš„åŸå§‹è¾“å…¥
- **ç¤ºä¾‹**:
  ```json
  {
    "centered-text": "text-align: center;",
    "red-text": "color: red;"
  }
  ```

---

### é…ç½®æ–‡ä»¶

#### `train_config.yaml`
- **ä½œç”¨**: LLaMA-Factory è®­ç»ƒé…ç½®
- **æ ¼å¼**: YAML
- **å†…å®¹**: 
  - æ¨¡å‹è·¯å¾„
  - æ•°æ®é›†é…ç½®
  - LoRA å‚æ•°
  - è®­ç»ƒå‚æ•°
- **ä½¿ç”¨**: `train_cli.py` è¯»å–æ­¤é…ç½®

---

### Shell è„šæœ¬

æ‰€æœ‰è„šæœ¬ä½äº `scripts/` ç›®å½•ï¼Œè¯¦è§ [è„šæœ¬ä½¿ç”¨æŒ‡å—](SCRIPTS_GUIDE.md)

---

## ğŸ’¾ è¾“å‡ºç›®å½•

### `css_assistant_model/`
è®­ç»ƒå®Œæˆåçš„æ¨¡å‹æ–‡ä»¶

#### æ–‡ä»¶è¯´æ˜

##### `adapter_model.safetensors`
- **ä½œç”¨**: LoRA æƒé‡æ–‡ä»¶
- **å¤§å°**: çº¦ 50-100 MB
- **æ ¼å¼**: SafeTensorsï¼ˆå®‰å…¨çš„å¼ é‡å­˜å‚¨æ ¼å¼ï¼‰
- **ç”¨é€”**: åŠ è½½å¾®è°ƒåçš„æ¨¡å‹

##### `adapter_config.json`
- **ä½œç”¨**: LoRA é…ç½®æ–‡ä»¶
- **å†…å®¹**:
  ```json
  {
    "r": 8,
    "lora_alpha": 16,
    "target_modules": ["q_proj", "k_proj", ...],
    "lora_dropout": 0.05,
    "bias": "none",
    "task_type": "CAUSAL_LM"
  }
  ```

##### `tokenizer_config.json` & `tokenizer.json`
- **ä½œç”¨**: åˆ†è¯å™¨é…ç½®å’Œè¯è¡¨
- **ç”¨é€”**: æ–‡æœ¬ç¼–ç å’Œè§£ç 

##### `checkpoint-*/`
- **ä½œç”¨**: è®­ç»ƒæ£€æŸ¥ç‚¹
- **å†…å®¹**: 
  - æ¨¡å‹æƒé‡
  - ä¼˜åŒ–å™¨çŠ¶æ€
  - è®­ç»ƒæ­¥æ•°
- **ç”¨é€”**: æ¢å¤è®­ç»ƒã€é€‰æ‹©æœ€ä½³æ¨¡å‹

---

## ğŸ è™šæ‹Ÿç¯å¢ƒ

### `train_env/`
- **ä½œç”¨**: ä¸»è®­ç»ƒç¯å¢ƒ
- **Python**: 3.11
- **åŒ…**: torch, transformers, peft, datasets, accelerate
- **åˆ›å»º**: `setup_and_train.sh` è‡ªåŠ¨åˆ›å»º
- **çŠ¶æ€**: Git å·²å¿½ç•¥

### `llama_env/`
- **ä½œç”¨**: LLaMA-Factory ç¯å¢ƒ
- **ç”¨é€”**: è¿è¡Œ LLaMA-Factory Web UI
- **çŠ¶æ€**: Git å·²å¿½ç•¥

### `venv/`
- **ä½œç”¨**: å…¶ä»–è™šæ‹Ÿç¯å¢ƒ
- **çŠ¶æ€**: Git å·²å¿½ç•¥

---

## ğŸ­ ç¬¬ä¸‰æ–¹æ¡†æ¶

### `LLaMA-Factory/`
- **ä½œç”¨**: LLaMA-Factory å¾®è°ƒæ¡†æ¶
- **æ¥æº**: https://github.com/hiyouga/LLaMA-Factory
- **åŠŸèƒ½**: 
  - Web UI è®­ç»ƒç•Œé¢
  - å¤šç§å¾®è°ƒæ–¹æ³•
  - æ•°æ®é›†ç®¡ç†
- **çŠ¶æ€**: Git å·²å¿½ç•¥
- **ä½¿ç”¨**: å¯é€‰ï¼Œæä¾› Web UI æ–¹å¼è®­ç»ƒ

---

## ğŸ“ æ—¥å¿—æ–‡ä»¶

### `training.log`
- **ä½œç”¨**: è®­ç»ƒæ—¥å¿—ï¼ˆåå°è¿è¡Œæ—¶ï¼‰
- **åˆ›å»º**: `resume_training.sh` å¯åŠ¨åå°è®­ç»ƒæ—¶
- **å†…å®¹**: 
  - è®­ç»ƒè¿›åº¦
  - æŸå¤±å€¼
  - å­¦ä¹ ç‡
  - é”™è¯¯ä¿¡æ¯
- **æŸ¥çœ‹**: `tail -f training.log`

---

## ğŸš« å¿½ç•¥çš„æ–‡ä»¶

ä»¥ä¸‹æ–‡ä»¶/ç›®å½•å·²åœ¨ `.gitignore` ä¸­å¿½ç•¥ï¼š

### è™šæ‹Ÿç¯å¢ƒ
- `**/venv/`
- `**/env/`
- `**/train_env/`
- `**/llama_env/`

### æ¨¡å‹æ–‡ä»¶
- `*.bin`
- `*.safetensors`
- `*.pth`
- `*.ckpt`

### æ¡†æ¶å’Œç¼“å­˜
- `LLaMA-Factory/`
- `.cache/`
- `huggingface/`
- `__pycache__/`

### è¾“å‡ºç›®å½•
- `css_assistant_model/`ï¼ˆè®­ç»ƒå®Œæˆåå¯é€‰æ‹©æ€§æäº¤ï¼‰
- `checkpoint-*/`

### æ—¥å¿—
- `*.log`
- `training.log`

---

## ğŸ“Š æ–‡ä»¶å¤§å°å‚è€ƒ

| æ–‡ä»¶/ç›®å½• | å¤§å° | è¯´æ˜ |
|----------|------|------|
| `training_data.json` | 1.3 MB | è®­ç»ƒæ•°æ® |
| `css_classes.json` | 145 KB | CSS ç±»å®šä¹‰ |
| `simple_train.py` | 5 KB | è®­ç»ƒè„šæœ¬ |
| `process_data.py` | 20 KB | æ•°æ®å¤„ç† |
| `train_env/` | 2-3 GB | è™šæ‹Ÿç¯å¢ƒ |
| `css_assistant_model/` | 50-200 MB | è®­ç»ƒè¾“å‡º |
| `LLaMA-Factory/` | 500 MB | æ¡†æ¶ä»£ç  |
| Qwen2.5-3B æ¨¡å‹ | 6 GB | ä¸‹è½½çš„åŸºåº§æ¨¡å‹ |

**æ€»ç£ç›˜å ç”¨**: çº¦ 10-15 GB

---

## ğŸ”„ æ–‡ä»¶ç”Ÿå‘½å‘¨æœŸ

### 1. åˆå§‹çŠ¶æ€
```
finetune/
â”œâ”€â”€ README.md
â”œâ”€â”€ docs/
â”œâ”€â”€ scripts/
â”œâ”€â”€ simple_train.py
â”œâ”€â”€ process_data.py
â”œâ”€â”€ css_classes.json
â””â”€â”€ training_data.json
```

### 2. è¿è¡Œ `setup_and_train.sh` å
```
finetune/
â”œâ”€â”€ ... (ä¸Šè¿°æ–‡ä»¶)
â”œâ”€â”€ train_env/          # æ–°åˆ›å»º
â””â”€â”€ css_assistant_model/ # æ–°åˆ›å»ºï¼ˆç©ºï¼‰
```

### 3. è®­ç»ƒè¿‡ç¨‹ä¸­
```
css_assistant_model/
â”œâ”€â”€ checkpoint-500/     # ç¬¬ 500 æ­¥
â”œâ”€â”€ checkpoint-1000/    # ç¬¬ 1000 æ­¥
â””â”€â”€ checkpoint-1500/    # ç¬¬ 1500 æ­¥
```

### 4. è®­ç»ƒå®Œæˆå
```
css_assistant_model/
â”œâ”€â”€ checkpoint-1500/    # æœ€åä¸€ä¸ªæ£€æŸ¥ç‚¹
â”œâ”€â”€ adapter_model.safetensors
â”œâ”€â”€ adapter_config.json
â”œâ”€â”€ tokenizer_config.json
â””â”€â”€ tokenizer.json
```

---

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [ä¸» README](../README.md)
- [å¿«é€Ÿå¼€å§‹](QUICK_START.md)
- [è„šæœ¬æŒ‡å—](SCRIPTS_GUIDE.md)
- [è®­ç»ƒçŠ¶æ€](TRAINING_STATUS.md)

---

**æ¸…æ™°çš„ç»“æ„ï¼Œé«˜æ•ˆçš„å¼€å‘ï¼** ğŸ“
