#!/usr/bin/env python3
"""
CSS åŠ©æ‰‹æ¨¡å‹æµ‹è¯•è„šæœ¬
ä½¿ç”¨è®­ç»ƒå¥½çš„ LoRA æ¨¡å‹ç”Ÿæˆ CSS ä»£ç 
"""

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import torch

# é…ç½®
BASE_MODEL = "Qwen/Qwen2.5-1.5B-Instruct"
ADAPTER_PATH = "./css_assistant_model"

print("=" * 60)
print("CSS åŠ©æ‰‹æ¨¡å‹æµ‹è¯•")
print("=" * 60)
print("\n[1/2] åŠ è½½æ¨¡å‹ä¸­...")

# åŠ è½½åŸºåº§æ¨¡å‹
base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    device_map="auto",
    torch_dtype=torch.float16,
    trust_remote_code=True
)

# åŠ è½½åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)

# åŠ è½½ LoRA æƒé‡
model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)
model.eval()

print("âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")
print(f"  åŸºåº§æ¨¡å‹: {BASE_MODEL}")
print(f"  LoRA æƒé‡: {ADAPTER_PATH}")

def generate_css(prompt, max_length=512, temperature=0.7, top_p=0.9):
    """
    ç”Ÿæˆ CSS ä»£ç 
    
    å‚æ•°:
        prompt: ç”¨æˆ·æç¤ºè¯
        max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
        temperature: æ¸©åº¦å‚æ•° (0.1-1.0, è¶Šä½è¶Šç¡®å®š)
        top_p: nucleus sampling å‚æ•°
    """
    # æ ¼å¼åŒ–ä¸º Qwen å¯¹è¯æ ¼å¼
    text = f"<|im_start|>system\nä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ CSS åŠ©æ‰‹ã€‚<|im_end|>\n<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
    
    # åˆ†è¯
    inputs = tokenizer(text, return_tensors="pt").to(model.device)
    
    # ç”Ÿæˆ
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=max_length,
            temperature=temperature,
            top_p=top_p,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    # è§£ç 
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # æå– assistant å›å¤éƒ¨åˆ†
    if "<|im_start|>assistant\n" in result:
        return result.split("<|im_start|>assistant\n")[-1].strip()
    return result

print("\n[2/2] è¿è¡Œæµ‹è¯•ç”¨ä¾‹...")
print("=" * 60)

# æµ‹è¯•ç”¨ä¾‹
test_prompts = [
    "ç”Ÿæˆä¸€ä¸ªå±…ä¸­çš„çº¢è‰²æ–‡å­—çš„ CSS ç±»",
    "åˆ›å»ºä¸€ä¸ªåœ†è§’è¾¹æ¡†çš„æŒ‰é’®æ ·å¼",
    "å†™ä¸€ä¸ªå“åº”å¼å‚ç›´å±…ä¸­çš„å¸ƒå±€",
]

for i, prompt in enumerate(test_prompts, 1):
    print(f"\n[æµ‹è¯• {i}/{len(test_prompts)}] {prompt}")
    print("-" * 60)
    
    try:
        result = generate_css(prompt, temperature=0.7)
        print(result)
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
    
    print()

print("=" * 60)
print("âœ“ æµ‹è¯•å®Œæˆ")
print("\nğŸ’¡ ä½¿ç”¨æç¤º:")
print("  - ä¿®æ”¹ test_prompts åˆ—è¡¨æ·»åŠ è‡ªå·±çš„æµ‹è¯•ç”¨ä¾‹")
print("  - è°ƒæ•´ temperature (0.1-1.0) æ§åˆ¶ç”Ÿæˆéšæœºæ€§")
print("  - è°ƒæ•´ max_length æ§åˆ¶ç”Ÿæˆé•¿åº¦")
print("=" * 60)

# äº¤äº’æ¨¡å¼
print("\nè¿›å…¥äº¤äº’æ¨¡å¼ï¼ˆè¾“å…¥ 'quit' é€€å‡ºï¼‰:")
print("-" * 60)

while True:
    try:
        user_input = input("\nè¯·è¾“å…¥æç¤ºè¯: ").strip()
        
        if user_input.lower() in ['quit', 'exit', 'q']:
            print("å†è§ï¼")
            break
        
        if not user_input:
            continue
        
        print("\nç”Ÿæˆä¸­...")
        result = generate_css(user_input, temperature=0.7)
        print("-" * 60)
        print(result)
        print("-" * 60)
        
    except KeyboardInterrupt:
        print("\n\nå†è§ï¼")
        break
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
