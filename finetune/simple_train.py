#!/usr/bin/env python3
"""
ç®€åŒ–çš„ CSS åŠ©æ‰‹æ¨¡å‹å¾®è°ƒè„šæœ¬ - Mac ä¼˜åŒ–ç‰ˆæœ¬
ä½¿ç”¨ Transformers + PEFT (LoRA) ç›´æ¥è®­ç»ƒ
é’ˆå¯¹ Mac æ€§èƒ½ä¼˜åŒ–ï¼Œä½¿ç”¨è½»é‡åŒ–é…ç½®
"""

import json
import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer
)
from peft import LoraConfig, get_peft_model
from datasets import load_dataset
import os

# é…ç½®
MODEL_NAME = "Qwen/Qwen2.5-1.5B-Instruct"  # åŸºåº§æ¨¡å‹ï¼ˆæ›´å°çš„ 1.5B æ¨¡å‹ï¼‰
DATA_FILE = "training_data.json"  # è®­ç»ƒæ•°æ®
OUTPUT_DIR = "./css_assistant_model"  # è¾“å‡ºç›®å½•
MAX_LENGTH = 512  # æœ€å¤§åºåˆ—é•¿åº¦

# LoRA é…ç½®ï¼ˆè½»é‡åŒ–ï¼Œé€‚åˆ Macï¼‰
LORA_R = 5  # LoRA ç§©ï¼ˆä»8é™åˆ°5ï¼Œå‡å°‘å¯è®­ç»ƒå‚æ•°ï¼‰
LORA_ALPHA = 32  # LoRA alphaï¼ˆå¢å¤§ç¼©æ”¾å› å­ï¼‰
LORA_DROPOUT = 0.05  # Dropout

# è®­ç»ƒé…ç½®ï¼ˆMac ä¼˜åŒ–ï¼‰
BATCH_SIZE = 2  # æ‰¹æ¬¡å¤§å°ï¼ˆä»4é™åˆ°2ï¼ŒèŠ‚çœå†…å­˜ï¼‰
GRADIENT_ACCUMULATION_STEPS = 4  # æ¢¯åº¦ç´¯ç§¯
LEARNING_RATE = 2e-4  # å­¦ä¹ ç‡
NUM_EPOCHS = 3  # è®­ç»ƒè½®æ•°

print("=" * 60)
print("CSS åŠ©æ‰‹æ¨¡å‹å¾®è°ƒ (Mac ä¼˜åŒ–ç‰ˆ - 1.5B å°æ¨¡å‹)")
print("=" * 60)

# 1. åŠ è½½åˆ†è¯å™¨
print("\n[1/5] åŠ è½½åˆ†è¯å™¨...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
print(f"âœ“ åˆ†è¯å™¨åŠ è½½å®Œæˆ")

# 2. åŠ è½½è®­ç»ƒæ•°æ®
print("\n[2/5] åŠ è½½è®­ç»ƒæ•°æ®...")
dataset = load_dataset("json", data_files=DATA_FILE)["train"]
print(f"âœ“ åŠ è½½äº† {len(dataset)} æ¡è®­ç»ƒæ•°æ®")

# 3. æ•°æ®é¢„å¤„ç†
print("\n[3/5] æ•°æ®é¢„å¤„ç†...")
def preprocess_function(examples):
    """é¢„å¤„ç†å‡½æ•° - ç®€åŒ–ç‰ˆ"""
    instruction = examples['instruction']
    input_text = examples.get('input', '')
    output = examples['output']
    
    # æ ¼å¼åŒ–ä¸º Qwen æ ¼å¼
    if input_text:
        text = f"<|im_start|>system\nä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ CSS åŠ©æ‰‹ã€‚<|im_end|>\n<|im_start|>user\n{instruction}\n{input_text}<|im_end|>\n<|im_start|>assistant\n{output}<|im_end|>"
    else:
        text = f"<|im_start|>system\nä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ CSS åŠ©æ‰‹ã€‚<|im_end|>\n<|im_start|>user\n{instruction}<|im_end|>\n<|im_start|>assistant\n{output}<|im_end|>"
    
    # ç¼–ç è¾“å…¥
    inputs = tokenizer(
        text,
        truncation=True,
        max_length=MAX_LENGTH,
        padding="max_length",
        return_tensors=None  # å¿…é¡»ä¸º None
    )
    inputs["labels"] = inputs["input_ids"].copy()
    return inputs

tokenized_dataset = dataset.map(preprocess_function, remove_columns=dataset.column_names)
print(f"âœ“ æ•°æ®é›†é¢„å¤„ç†å®Œæˆ")

# 4. LoRA é…ç½®
print("\n[4/5] é…ç½® LoRA...")
lora_config = LoraConfig(
    r=LORA_R,
    lora_alpha=LORA_ALPHA,
    target_modules=["q_proj", "v_proj"],  # ä»… 2 ä¸ªæ¨¡å—ï¼ˆè½»é‡åŒ–ï¼‰
    lora_dropout=LORA_DROPOUT,
    bias="none",
    task_type="CAUSAL_LM"
)

# åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    device_map="auto",  # è‡ªåŠ¨åˆ†é…åˆ° MPSï¼ˆApple GPUï¼‰
    torch_dtype=torch.float16,  # ä½¿ç”¨ float16 ç²¾åº¦
    trust_remote_code=True
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()  # æ‰“å°å¯è®­ç»ƒå‚æ•°
print(f"âœ“ LoRA é…ç½®å®Œæˆ")

# 5. è®­ç»ƒé…ç½®
print("\n[5/5] é…ç½®è®­ç»ƒå‚æ•°...")
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=BATCH_SIZE,  # Mac ä¼˜åŒ–ï¼šæ‰¹æ¬¡å¤§å° 2
    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
    num_train_epochs=NUM_EPOCHS,
    learning_rate=LEARNING_RATE,
    logging_dir="./logs",
    logging_steps=10,
    save_strategy="epoch"  # æ¯ä¸ª epoch ä¿å­˜ä¸€æ¬¡
)

print(f"âœ“ è®­ç»ƒå‚æ•°é…ç½®å®Œæˆ")
print(f"  æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")
print(f"  æ¢¯åº¦ç´¯ç§¯: {GRADIENT_ACCUMULATION_STEPS}")
print(f"  æœ‰æ•ˆæ‰¹æ¬¡: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}")
print(f"  å­¦ä¹ ç‡: {LEARNING_RATE}")
print(f"  è®­ç»ƒè½®æ•°: {NUM_EPOCHS}")
print(f"  LoRA ç§©: {LORA_R} (è½»é‡åŒ–é…ç½®)")

# å¼€å§‹è®­ç»ƒ
print("\nå¼€å§‹è®­ç»ƒ...")
print("=" * 60)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset
)

trainer.train()

# ä¿å­˜æ¨¡å‹
print("\nä¿å­˜æ¨¡å‹...")
model.save_pretrained(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)

print("\n" + "=" * 60)
print("âœ“ è®­ç»ƒå®Œæˆï¼")
print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {OUTPUT_DIR}")
print("=" * 60)
print("\nğŸ’¡ æç¤ºï¼š")
print("  - æœ¬æ¬¡ä½¿ç”¨ Mac ä¼˜åŒ–é…ç½®ï¼ˆLoRA r=5, batch=2ï¼‰")
print("  - å¯è®­ç»ƒå‚æ•°çº¦å æ€»å‚æ•°çš„ 0.1%")
print("  - å¦‚æœå†…å­˜ä»ä¸è¶³ï¼Œå¯å°† BATCH_SIZE æ”¹ä¸º 1")

